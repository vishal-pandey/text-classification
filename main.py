# -*- coding: utf-8 -*-
"""precily.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yGjIbw28MF_gXYnTLkHruAb3kXTbJeb9

#Text Classification for single layer neural network
"""

# Library imports which are needed

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math

# Importing data from CSV
df = pd.read_csv('bbc-text.csv')

# Data Frame generated
df

# Importing label encoder from sklearn for encoding labels
from sklearn.preprocessing import LabelEncoder
lab = LabelEncoder()
y = lab.fit_transform(df.iloc[:,0].values)

X = df.iloc[:, 1].values

# X[0]

# importing libraries for text preprocessing
import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem.porter import PorterStemmer

ps = PorterStemmer()

# Processing text converting all text to lower case and removing stopwords
processed_list = []
for i in range(len(X)):
  article = re.sub('[^a-zA-Z]', ' ', X[i])
  article = article.lower()
  article = article.split()
  article = [ps.stem(token) for token in article if not token in stopwords.words('english')]
  article = " ".join(article)
  processed_list.append(article)

# Vectorising text for training of the neural network
cv = CountVectorizer(max_features=3000)
X = cv.fit_transform(processed_list).toarray()
len(cv.get_feature_names())

# Splitting the data into training and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)



"""## Using Neural Network for developing model for the classification for the text"""

# Sigmoid function to convert list's all values to its sigmoid 
def sigmoid(z):
  x = []
  for i in z:
    try:
      x.append(1/(1+np.exp(-i[0])))
    except OverflowError:
      print(i)
      break
      return
  return np.asarray(x)

# One Hot Encoder for encoding labels
def oneHotEncoder(x):
  return np.asarray([int(x == e) for e in range(5)])

# Function for generating result after processsing the model
def result_test(i):
  return list(sigmoid(X_test[i]@w[:])).index(max(sigmoid(X_test[i]@w[:])))

# Learning rate
learning_rate = 0.005

# Generating random weights for the training
w = np.random.random((5, 3000, 1))

# w.shape

# Here we used single layer neural network in which we used Mean Squared Error as a loss function, in each iteration we trained our weights
# for a single input
progress = 0
for i in range(len(X_train)):
  for j in range(len(w)):
    w[j]+=(learning_rate * X_train[i] * 2 * (oneHotEncoder(y_train[i]) - sigmoid(X_train[i]@w[:]))[j]).reshape((3000,1))
  if progress < i/len(X_train)*100:
    progress += 1
    print(progress, end=" ")

# Testing for the accuracy on the test set
acuracy = 0
for i in range(len(X_test)):
  if result_test(i) == y_test[i]:
    acuracy += 1

print(acuracy/len(X_test))

